{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports and Initializing the model**"
      ],
      "metadata": {
        "id": "V3U8Fs8NjeH0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taLTYhxGTZrS"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import gradio as gr\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from helper import get_openai_api_key\n",
        "import nest_asyncio\n",
        "\n",
        "OPENAI_API_KEY = get_openai_api_key()\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Initialize the OpenAI model\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompt**"
      ],
      "metadata": {
        "id": "oM2KcFomjqyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are a highly intelligent document assistant. Your answers must be strictly based on the provided context.\n",
        "    If the information is not found in the context, clearly state: \"The information is not available in the document.\"\n",
        "\n",
        "    Your goal is to:\n",
        "    1. Understand the user's question thoroughly and break it into subparts if complex.\n",
        "    2. Retrieve the most relevant content from all provided documents.\n",
        "    3. Combine information across documents when necessary to provide accurate and coherent answers.\n",
        "    4. If the question involves locating specific terms (e.g., \"pages where the word 'overfitting' appears\"), identify and provide precise details.\n",
        "    5. Handle differentiations, tabular data, and return results with well-formatted tables when applicable.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "    Question: {input}\n",
        "    Answer the question thoughtfully and accurately based on the provided context:\n",
        "    \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "AUgdvtd1UYXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embeddings, Chunking and Vector store**"
      ],
      "metadata": {
        "id": "pHTPRbtTj8Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process PDFs and create embeddings\n",
        "def process_pdfs(pdf_paths):\n",
        "    embeddings_model = OpenAIEmbeddings()\n",
        "    all_documents = []\n",
        "\n",
        "    for pdf_path in pdf_paths:\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        docs = loader.load()\n",
        "        all_documents.extend(docs)\n",
        "\n",
        "    # Create chunks using a text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    final_documents = text_splitter.split_documents(all_documents)\n",
        "\n",
        "    # Process embeddings for all documents\n",
        "    vector_store = FAISS.from_documents(final_documents, embeddings_model)\n",
        "    return vector_store\n",
        "\n",
        "# Function to query the vector store\n",
        "def query_vector_store(vector_store, user_query):\n",
        "    retriever = vector_store.as_retriever()\n",
        "    document_chain = create_stuff_documents_chain(llm, prompt_template)\n",
        "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "    response = retrieval_chain.invoke({'input': user_query})\n",
        "\n",
        "    # Ensure response is strictly based on the document context\n",
        "    if response.get('answer', '').strip():\n",
        "        return response['answer']\n",
        "    else:\n",
        "        return \"The information is not available in the document.\""
      ],
      "metadata": {
        "id": "crnkbClCUd2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradio interface**"
      ],
      "metadata": {
        "id": "FCA41pPBkAaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store, chat_history = None, []\n",
        "\n",
        "def handle_pdfs(pdfs=None, query=None):\n",
        "    global vector_store, chat_history\n",
        "    if pdfs:\n",
        "        vector_store = process_pdfs([pdf.name for pdf in pdfs])\n",
        "        chat_history.clear()\n",
        "    elif query:\n",
        "        if not vector_store:\n",
        "            chat_history.append({\"role\": \"user\", \"content\": query})\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": \"Please upload PDFs first.\"})\n",
        "        else:\n",
        "            answer = query_vector_store(vector_store, query)\n",
        "            chat_history.append({\"role\": \"user\", \"content\": query})\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "    return chat_history\n",
        "\n",
        "def refresh_chat():\n",
        "    global vector_store, chat_history\n",
        "    vector_store, chat_history = None, []\n",
        "    return []  # Return an empty list to clear the chatbox\n",
        "\n",
        "# Custom CSS for better UX\n",
        "css = \"\"\"\n",
        ".chatbot .user-message { color: #1E88E5; }\n",
        ".chatbot .assistant-message { color: #28A745; }\n",
        ".chatbot { height: 500px; overflow-y: scroll; }\n",
        ".row { display: flex; justify-content: space-between; }\n",
        ".column { flex: 1; margin-right: 10px; }\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(theme=\"Yntec/HaleyCH_Theme_Orange_Green\") as app:\n",
        "    gr.HTML(f\"<style>{css}</style>\")\n",
        "    gr.Markdown(\"\"\"\n",
        "        <div style=\"text-align: center;\">\n",
        "            <h1>ðŸ“– Multi-PDF Question-Answer Assistant</h1>\n",
        "            <p>An interactive application to upload multiple PDFs and ask complex questions about their content.</p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            pdf_upload = gr.File(label=\"Upload PDFs\", file_types=[\".pdf\"], file_count=\"multiple\")\n",
        "            refresh_button = gr.Button(\"Refresh\", variant=\"secondary\")\n",
        "        with gr.Column(scale=2):\n",
        "            query_input = gr.Textbox(label=\"Your Question\", placeholder=\"Type your question here...\", lines=1)\n",
        "            ask_button = gr.Button(\"Ask Question\")\n",
        "\n",
        "    with gr.Row():\n",
        "        chatbox = gr.Chatbot(label=\"Chat with your PDFs\", type=\"messages\")\n",
        "\n",
        "    pdf_upload.change(lambda pdfs: handle_pdfs(pdfs=pdfs), inputs=pdf_upload, outputs=chatbox)\n",
        "    ask_button.click(lambda query: handle_pdfs(query=query), inputs=query_input, outputs=chatbox)\n",
        "    refresh_button.click(refresh_chat, inputs=None, outputs=chatbox)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.launch(share=True)"
      ],
      "metadata": {
        "id": "8Snlo4Vns-Sp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}